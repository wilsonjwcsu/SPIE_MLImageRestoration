{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3a1cb62-2f11-45be-a12d-6c7432c55d0a",
   "metadata": {},
   "source": [
    "# Deconvolution demo.\n",
    "SPIE Short course on Machine Learning for Image Restoration.  \n",
    "Author: Jesse Wilson (jesse.wilson@colostate.edu).\n",
    "\n",
    "Walk through training and evaluation of a convolutional network for denoising. This code is provided for educational purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f933f86e-1948-41a0-a98c-25cb5c76bffa",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a07d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from torchvision.transforms.functional import gaussian_blur\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from torch import fft\n",
    "\n",
    "# get available GPU \n",
    "# supports NVIDIA (CUDA), Intel (XPU), and Apple (MPS)\n",
    "# (CAUTION: AI-generated code -- NOT validated on all systems!)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif hasattr(torch,\"xpu\") and torch.xpu_is_available():\n",
    "    device = torch.device(\"xpu:0\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Selected device: {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be014ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a dataset\n",
    "\n",
    "batch_size=64\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Pad((18,18,18,18))])\n",
    "dataset_train = datasets.MNIST(root='data',train=True,download=True,transform=transform)\n",
    "dataset_val = datasets.MNIST(root='data',train=False,download=True,transform=transform)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size, shuffle=False)\n",
    "\n",
    "img = next(iter(dataloader_train))[0][1].squeeze()\n",
    "plt.imshow(img)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f45cb91-ab0b-4467-97f3-605b8a5e77be",
   "metadata": {},
   "source": [
    "# Set up a class to simulate the optical system\n",
    "Assumes spatially-invariant PSF resulting from a 4f system. Incoherent imaging. For more information, see [Voelz _Computational Fourier Optics: A MATLAB Tutorial_](https://www.spiedigitallibrary.org/ebooks/TT/Computational-Fourier-Optics-A-MATLAB-Tutorial/eISBN-9780819482051/10.1117/3.858456)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522ea0e7-1afc-4398-843e-c4467f5ab2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a (incoherent) PSF\n",
    "class ForwardModel( nn.Module ):\n",
    "    def __init__(self, pupil_width=0.5, ny=64, nx=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # set up sampling grid\n",
    "        dx = 2 / nx\n",
    "        x = torch.linspace(-1,1-dx,nx)\n",
    "        y = x\n",
    "        X,Y = torch.meshgrid(x,y)\n",
    "        R=torch.sqrt(X**2+Y**2)\n",
    "        CTF = R < torch.Tensor([pupil_width])\n",
    "\n",
    "        # set up OTF\n",
    "        CSF = fft.fftshift(fft.ifft2(fft.ifftshift(CTF)))\n",
    "        PSF = CSF.abs()**2\n",
    "        OTF = fft.fftshift(fft.fft2(fft.ifftshift(PSF)))\n",
    "        OTF = OTF / torch.max(OTF.abs())\n",
    "        self.OTF = OTF\n",
    "\n",
    "    def forward(self, x):\n",
    "        # convolve image with PSF by product with OTF in Fourier domain\n",
    "        img_f = fft.fftshift(fft.fft2(fft.ifftshift(x)))\n",
    "        img2_f = img_f * self.OTF\n",
    "        img2 = fft.fftshift(fft.ifft2(fft.ifftshift(img2_f)))\n",
    "        img2 = img2.real\n",
    "        img2 = F.relu(img2)\n",
    "        return img2\n",
    "\n",
    "x = dataset_train[0][0]\n",
    "forwardModel = ForwardModel(0.2)\n",
    "y = forwardModel(x)\n",
    "plt.subplot(121)\n",
    "plt.imshow(x.squeeze())\n",
    "plt.axis(\"off\")\n",
    "plt.title('object')\n",
    "plt.subplot(122)\n",
    "plt.imshow(y.squeeze())\n",
    "plt.axis('off')\n",
    "plt.title('image')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392081a3-7c90-402e-b69a-375e7be96206",
   "metadata": {},
   "source": [
    "# Neural network definition and quick passthrough test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d5f058-705c-4b07-a9a9-c98bceb37ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple u-net\n",
    "# Here we use  average pooling and interpolation instead of strided convolutions,\n",
    "# as it is less prone to grid-like artifacts in the output\n",
    "class UNet( nn.Module ):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.downconv1 = nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 3, padding=1) # 64x64\n",
    "        self.downconv2 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, padding=1) # 32x32\n",
    "        self.downconv3 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, padding=1) # 16x16\n",
    "        self.downconv4 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, padding=1) # 8x8\n",
    "\n",
    "        self.upconv1 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, padding=1) # 4x4\n",
    "        self.upconv2 = nn.Conv2d(in_channels = 64, out_channels = 32, kernel_size = 3, padding=1) # 8x8\n",
    "        self.upconv3 = nn.Conv2d(in_channels = 64, out_channels = 32, kernel_size = 3, padding=1) # 16x16\n",
    "        self.upconv4 = nn.Conv2d(in_channels = 64, out_channels = 32, kernel_size = 3, padding=1) # 32x32\n",
    "\n",
    "        self.outconv = nn.Conv2d(in_channels = 64, out_channels = 1, kernel_size = 3, padding=1, bias=False) # 64x64\n",
    "\n",
    "        # initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                torch.nn.init.kaiming_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.)     \n",
    "\n",
    "    def forward(self, x):\n",
    "        featdown1 = F.leaky_relu(self.downconv1(x)) # 64x64\n",
    "        featdown2 = F.leaky_relu(self.downconv2(F.avg_pool2d(featdown1,2))) # 32x32\n",
    "        featdown3 = F.leaky_relu(self.downconv3(F.avg_pool2d(featdown2,2))) # 16x16\n",
    "        featdown4 = F.leaky_relu(self.downconv4(F.avg_pool2d(featdown3,2))) # 8x8\n",
    "\n",
    "        featup1 = F.interpolate(F.leaky_relu(self.upconv1(F.avg_pool2d(featdown4,2))),scale_factor=2,mode='bilinear') # 8x8\n",
    "        skip1 = torch.cat((featdown4, featup1),1) # 8x8\n",
    "        featup2 = F.interpolate(F.leaky_relu(self.upconv2(skip1)), scale_factor=2,mode='bilinear') # 16x16\n",
    "        skip2 = torch.cat((featdown3, featup2),1) # 16x16\n",
    "        featup3 = F.interpolate(F.leaky_relu(self.upconv3(skip2)), scale_factor=2, mode='bilinear') # 32x32\n",
    "        skip3 = torch.cat((featdown2, featup3),1) # 32x32\n",
    "        featup4 = F.interpolate(F.leaky_relu(self.upconv4(skip3)), scale_factor=2, mode='bilinear') # 64x64\n",
    "        skip4 = torch.cat((featdown1, featup4),1) # 64x64\n",
    "        out = self.outconv(skip4) # 64x64\n",
    "        \n",
    "        return out\n",
    "\n",
    "net = UNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e33b50e-280c-438d-bba5-149d84d2d8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass a batch of images through the network\n",
    "img = next(iter(dataloader_train))[0][1]\n",
    "\n",
    "xhat = net(img.to(device).unsqueeze(0))\n",
    "print(xhat.shape)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(img[0].cpu().squeeze())\n",
    "plt.colorbar()\n",
    "plt.title('network input')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(xhat[0].detach().cpu().squeeze())\n",
    "plt.colorbar()\n",
    "plt.title('network output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175475b2-2092-4918-8c62-b3d9c20eaaa1",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ed8b2c-bf01-4d78-bd39-8ba562dea2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supervised training\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "loss_fn = nn.MSELoss()\n",
    "forwardModel.OTF = forwardModel.OTF.to(device)\n",
    "\n",
    "loss_train_vec = []\n",
    "\n",
    "# training loop\n",
    "n_epochs = 10\n",
    "i_iter = 0\n",
    "for epoch in range(1,n_epochs):\n",
    "    for data, label in dataloader_train:\n",
    "        x = data.to(device)\n",
    "        y = forwardModel(x)\n",
    "        xhat = net(x)\n",
    "    \n",
    "        # compare recovered image with ground truth\n",
    "        loss = loss_fn(xhat,x)\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        loss_train_vec += [loss.item()]\n",
    "        i_iter += 1\n",
    "    \n",
    "        # plot training/validation loss curves\n",
    "        if i_iter % 20 == 0:\n",
    "            clear_output(wait=True)\n",
    "            plt.figure(figsize=[8,8])\n",
    "            plt.subplot(241)\n",
    "            plt.imshow(x[0].cpu().squeeze())\n",
    "            plt.axis('off')\n",
    "            plt.title('object')\n",
    "            plt.subplot(242)\n",
    "            plt.imshow(y[0].cpu().squeeze())\n",
    "            plt.axis('off')\n",
    "            plt.title('image')\n",
    "            plt.subplot(243)\n",
    "            plt.imshow(xhat[0].detach().cpu().squeeze())\n",
    "            plt.axis(\"off\")\n",
    "            plt.title('net output')\n",
    "            plt.subplot(212)\n",
    "            plt.plot(loss_train_vec)\n",
    "            plt.title(f'epoch {epoch}, iter {i_iter}')\n",
    "            plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0b2097-fc3c-4f1d-8c72-78f2207854e6",
   "metadata": {},
   "source": [
    "# Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96dd9fcb-6cc3-4d3e-9fd2-581d0b84b4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your turn: change one thing above and run it again. A few ideas\n",
    "# - change pupil diameter\n",
    "# - change learning rate\n",
    "# - try L1 loss instead of MSE loss\n",
    "# - change neural network architecture\n",
    "# - add validation tracking to the training loop\n",
    "# - test on out-of-distribution images\n",
    "# - add noise to simulated image\n",
    "# - compare with Wiener deconvolution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce2b7ea-792a-4abb-903d-e9b64d7d3cac",
   "metadata": {},
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7855aab2-017e-4960-9ec4-f8c2afcba7f4",
   "metadata": {},
   "source": [
    "## Physics-informed (unsupervised) learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3c9c8b-4c41-4154-8377-37d7bde54ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientMagnitude(nn.Module):\n",
    "    def __init__(self,dev):\n",
    "        super().__init__()\n",
    "        self.sobel_x = torch.tensor([[1.,0.,-1.],[2.,0.,-2.],[1.,0.,1.]]).unsqueeze(0).unsqueeze(0).to(dev)\n",
    "        self.sobel_y = torch.tensor([[1.,2.,1.],[0.,0.,0.],[-1.,-2.,-1.]]).unsqueeze(0).unsqueeze(0).to(dev)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # calculate summed magnitude of the gradients of an image\n",
    "        grad_x = F.conv2d(x,self.sobel_x)\n",
    "        grad_y = F.conv2d(y,self.sobel_y)\n",
    "    \n",
    "        absgrad = -torch.abs(grad_x)**2 + torch.abs(grad_y)**2\n",
    "        return torch.mean(absgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53de62e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsupervised training\n",
    "# sometimes gets stuck producing blank output, may need to restart a few times\n",
    "# to obtain reasonable results\n",
    "net = UNet().to(device)\n",
    "\n",
    "# results are extremely sensitive to lr and lambda_gradmag\n",
    "lr = 0.001\n",
    "lambda_gradmag = 0.00001\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min',patience=10,cooldown=10)\n",
    "loss_fn = nn.L1Loss()\n",
    "loss_fn_gradmag = GradientMagnitude(device)\n",
    "forwardModel.OTF = forwardModel.OTF.to(device)\n",
    "\n",
    "loss_train_vec = []\n",
    "\n",
    "x = dataset_train[ind][0].to(device)\n",
    "y = forwardModel(x)\n",
    "\n",
    "# training loop\n",
    "n_epochs = 5000\n",
    "for epoch in range(1,n_epochs):\n",
    "    xhat = net(y.unsqueeze(0))\n",
    "\n",
    "    # pass output through imaging system\n",
    "    yhat = forwardModel(xhat)\n",
    "    \n",
    "\n",
    "    # compare recovered image with measurement\n",
    "    loss = loss_fn(yhat,y) + lambda_gradmag*loss_fn_gradmag(xhat)\n",
    "    #loss = loss_fn(yhat,y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #scheduler.step(loss)\n",
    "\n",
    "    loss_train_vec += [loss.item()]\n",
    "    \n",
    "    # plot training/validation loss curves\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=[8,8])\n",
    "    plt.subplot(241)\n",
    "    plt.imshow(x.cpu().squeeze())\n",
    "    plt.subplot(242)\n",
    "    plt.imshow(y.cpu().squeeze())\n",
    "    plt.subplot(243)\n",
    "    plt.imshow(xhat.detach().cpu().squeeze())\n",
    "    plt.subplot(244)\n",
    "    plt.imshow(yhat.detach().cpu().squeeze())\n",
    "    plt.subplot(212)\n",
    "    plt.plot(loss_train_vec)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce86619",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = randint(0,(len(dataset_train)-1))\n",
    "\n",
    "x = dataset_train[ind][0].to(device)\n",
    "y = forwardModel(x)\n",
    "with torch.no_grad():\n",
    "    xhat = net(y.unsqueeze(0))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.imshow(y.cpu().squeeze())\n",
    "plt.title('blurry')\n",
    "plt.subplot(132)\n",
    "plt.imshow(xhat.cpu().squeeze())\n",
    "plt.title('estimate')\n",
    "plt.subplot(133)\n",
    "plt.imshow(x.cpu().squeeze())\n",
    "plt.title('ground truth')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
