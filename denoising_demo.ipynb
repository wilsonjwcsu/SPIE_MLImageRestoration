{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64e195f8-f6c1-4c99-8d47-e1dc41f10840",
   "metadata": {},
   "source": [
    "# Denoising demo\n",
    "SPIE Short course on Machine Learning for Image Restoration.  \n",
    "Author: Jesse Wilson (jesse.wilson@colostate.edu).\n",
    "\n",
    "Walk through training and evaluation of a convolutional network for denoising. This code is provided for educational purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c92659-49fe-43e0-bbe7-fd796dfe8bc5",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a07d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denoising demonstration\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from random import randint\n",
    "\n",
    "# get available GPU \n",
    "# supports NVIDIA (CUDA), Intel (XPU), and Apple (MPS)\n",
    "# (CAUTION: AI-generated code -- NOT validated on all systems!)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif hasattr(torch,\"xpu\") and torch.xpu_is_available():\n",
    "    device = torch.device(\"xpu:0\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Selected device: {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be014ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a dataset\n",
    "\n",
    "batch_size=64\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Pad((18,18,18,18))])\n",
    "dataset_train = datasets.MNIST(root='data',train=True,download=True,transform=transform)\n",
    "dataset_val = datasets.MNIST(root='data',train=False,download=True,transform=transform)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size, shuffle=False)\n",
    "\n",
    "img = next(iter(dataloader_train))[0][1].squeeze()\n",
    "plt.imshow(img)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fbad93-9a32-48f3-a8cb-2781fc348188",
   "metadata": {},
   "source": [
    "# Set up a class to simulate the noise process\n",
    "Assume additive white Gaussian noise (drawn from randonm normal distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522ea0e7-1afc-4398-843e-c4467f5ab2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a noise process\n",
    "class ForwardModel( nn.Module ):\n",
    "    def __init__(self, sigma=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, x):\n",
    "        noise = torch.randn_like(x)\n",
    "        y = x + self.sigma * noise\n",
    "\n",
    "        return y\n",
    "\n",
    "# check that the model is working\n",
    "x = next(iter(dataloader_train))[0] # get a batch of images\n",
    "forwardModel = ForwardModel(0.2)\n",
    "y = forwardModel(x)\n",
    "plt.subplot(121)\n",
    "plt.imshow(x[0].squeeze())\n",
    "plt.title('original')\n",
    "plt.subplot(122)\n",
    "plt.imshow(y[0].squeeze())\n",
    "plt.title('noisy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aabcca-0d5e-4a25-91b7-5b9419cf7366",
   "metadata": {},
   "source": [
    "# Neural network definition and quick passthrough test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f32e0c-f8f5-43d0-9a1b-cb09fa3140be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple u-net with batchnorm\n",
    "class UNet( nn.Module ):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # convolutional encoder\n",
    "        self.downconv1 = nn.Sequential(nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 3, padding=1),nn.BatchNorm2d(32))\n",
    "        self.downconv2 = nn.Sequential(nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, padding=1),nn.BatchNorm2d(32))\n",
    "        self.downconv3 = nn.Sequential(nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, padding=1),nn.BatchNorm2d(32))\n",
    "        self.downconv4 = nn.Sequential(nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, padding=1),nn.BatchNorm2d(32))\n",
    "\n",
    "        # convolutional decoder\n",
    "        self.upconv1 = nn.Sequential(\n",
    "                    nn.ConvTranspose2d(in_channels = 32, out_channels = 32, kernel_size = 3, padding=1, stride=2, output_padding=1),\n",
    "                    nn.BatchNorm2d(32) )\n",
    "        self.upconv2 = nn.Sequential(\n",
    "                    nn.ConvTranspose2d(in_channels = 64, out_channels = 32, kernel_size = 3, padding=1, stride=2, output_padding=1),\n",
    "                    nn.BatchNorm2d(32) )\n",
    "        self.upconv3 = nn.Sequential(\n",
    "                        nn.ConvTranspose2d(in_channels = 64, out_channels = 32, kernel_size = 3, padding=1, stride=2, output_padding=1),\n",
    "                        nn.BatchNorm2d(32) )\n",
    "        self.upconv4 = nn.Sequential(\n",
    "                        nn.ConvTranspose2d(in_channels = 64, out_channels = 32, kernel_size = 3, padding=1, stride=2, output_padding=1),\n",
    "                        nn.BatchNorm2d(32) )\n",
    "\n",
    "        self.outconv = nn.Conv2d(in_channels = 64, out_channels = 1, kernel_size = 3, padding=1)\n",
    "\n",
    "        # initialize learnable parameters\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                nn.init.constant_(m.bias,0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pass image through encoder, retaining features at each level\n",
    "        featdown1 = F.leaky_relu(self.downconv1(x))\n",
    "        featdown2 = F.leaky_relu(self.downconv2(F.max_pool2d(featdown1,2)))\n",
    "        featdown3 = F.leaky_relu(self.downconv3(F.max_pool2d(featdown2,2)))\n",
    "        featdown4 = F.leaky_relu(self.downconv4(F.max_pool2d(featdown3,2)))\n",
    "\n",
    "        # pass through decoder\n",
    "        featup1 = F.leaky_relu(self.upconv1(F.max_pool2d(featdown4,2)))\n",
    "        skip1 = torch.cat((featdown4, featup1),1)\n",
    "        featup2 = F.leaky_relu(self.upconv2(skip1))\n",
    "        skip2 = torch.cat((featdown3, featup2),1)\n",
    "        featup3 = F.leaky_relu(self.upconv3(skip2))\n",
    "        skip3 = torch.cat((featdown2, featup3),1)\n",
    "        featup4 = F.leaky_relu(self.upconv4(skip3))\n",
    "        skip4 = torch.cat((featdown1, featup4),1)\n",
    "        out = self.outconv(skip4)\n",
    "        \n",
    "        return out\n",
    "\n",
    "net = UNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e33b50e-280c-438d-bba5-149d84d2d8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test run a noisy image through the network\n",
    "x = next(iter(dataloader_train))[0] # get a batch of images\n",
    "forwardModel = ForwardModel(0.2)\n",
    "y = forwardModel(x)\n",
    "with torch.no_grad():\n",
    "    xhat = net(y.to(device)) # run through network\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.imshow(x[0].squeeze())\n",
    "plt.title('original')\n",
    "plt.subplot(132)\n",
    "plt.imshow(y[0].squeeze())\n",
    "plt.title('noisy')\n",
    "plt.subplot(133)\n",
    "plt.imshow(xhat[0].cpu().squeeze())\n",
    "plt.title('network output')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6968bd7-81bb-40f9-a0a5-e4e6b74214aa",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ed8b2c-bf01-4d78-bd39-8ba562dea2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supervised training\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "loss_train_vec = []\n",
    "\n",
    "# training loop\n",
    "n_epochs = 10\n",
    "i_iter = 0\n",
    "for epoch in range(n_epochs):\n",
    "    for data, label in dataloader_train:\n",
    "    \n",
    "        x = data.to(device)\n",
    "        y = forwardModel(x)\n",
    "        xhat = net(y)\n",
    "\n",
    "        # compare recovered image with measurement\n",
    "        loss = loss_fn(xhat,x)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        loss_train_vec += [loss.item()]\n",
    "        i_iter += 1\n",
    "    \n",
    "        # plot training/validation loss curves\n",
    "        if i_iter % 200 == 0:\n",
    "            clear_output(wait=True)\n",
    "            plt.figure(figsize=[8,8])\n",
    "            plt.subplot(241)\n",
    "            plt.imshow(x[0].cpu().squeeze())\n",
    "            plt.subplot(242)\n",
    "            plt.imshow(y[0].cpu().squeeze())\n",
    "            plt.subplot(243)\n",
    "            plt.imshow(xhat[0].detach().cpu().squeeze())\n",
    "            plt.subplot(212)\n",
    "            plt.plot(loss_train_vec)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc46306-e1aa-41f0-8365-6131d86b92ba",
   "metadata": {},
   "source": [
    "# Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde57c7d-59e0-4b1c-822b-c65f2bc847df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your turn: change one thing above and run it again. A few ideas\n",
    "# - change batch size\n",
    "# - change learning rate\n",
    "# - change noise amount and re-train\n",
    "# - change neural network architecture\n",
    "# - add validation tracking to the training loop\n",
    "# - test on out-of-distribution images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d3d3f2-1c6f-4bc1-a4c9-3e82b043d11f",
   "metadata": {},
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56ece04-e3a2-416b-8245-46ed1112e1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# render a panel of images\n",
    "x = next(iter(dataloader_val))[0] # get a batch of images\n",
    "forwardModel = ForwardModel(0.2)\n",
    "y = forwardModel(x)\n",
    "with torch.no_grad():\n",
    "    xhat = net(y.to(device))\n",
    "\n",
    "plt.figure(figsize=[4,7])\n",
    "for ind in range(5):\n",
    "    plt.subplot(5,3,ind*3+1)\n",
    "    plt.imshow(x[ind].squeeze())\n",
    "    plt.axis('off')\n",
    "    if ind==0:\n",
    "        plt.title('original')\n",
    "    plt.subplot(5,3,ind*3+2)\n",
    "    plt.imshow(y[ind].squeeze())\n",
    "    plt.axis('off')\n",
    "    if ind==0:\n",
    "        plt.title('noisy')\n",
    "    plt.subplot(5,3,ind*3+3)\n",
    "    plt.imshow(xhat[ind].cpu().squeeze())\n",
    "    plt.axis('off')\n",
    "    if ind==0:\n",
    "        plt.title('denoised')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af269bed-95ea-4d6b-9c4d-dccfb8d7ba58",
   "metadata": {},
   "source": [
    "## Compare against Wiener filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857fb170-c4b9-49f9-a6f9-8baf2e840c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions for comparison metrics, assuming a single image at a time\n",
    "from skimage.metrics import structural_similarity\n",
    "def MSE(x, xhat):\n",
    "    return np.mean((x-xhat)**2)\n",
    "\n",
    "def MAE(x, xhat):\n",
    "    return np.mean(np.abs(x-xhat))\n",
    "\n",
    "def Pearson(x, y):\n",
    "    xbar = np.mean(x)\n",
    "    ybar = np.mean(y)\n",
    "\n",
    "    return np.sum( (x-xbar)*(y-ybar) ) / (np.sqrt(np.sum((x-xbar)**2))*np.sqrt(np.sum((y-ybar)**2)))\n",
    "\n",
    "def PSNR(x, xhat):\n",
    "    # note, this assumes images were normalized to a 0-1 range, and will need to change\n",
    "    # for images normalized to a 0-255 range\n",
    "    maxPossibleVal = 1.0\n",
    "    \n",
    "    mse = MSE(x,xhat)\n",
    "    return 10*np.log10(maxPossibleVal / mse)\n",
    "\n",
    "def SSIM(x,xhat):\n",
    "    minPossibleVal = 0.\n",
    "    maxPossibleVal = 1.0\n",
    "    return structural_similarity(x, xhat, data_range=(maxPossibleVal-minPossibleVal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c1a4f0-42e0-4be6-a840-d1192a7b2f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import restoration\n",
    "\n",
    "# Wiener filter parameters\n",
    "filt_len = 9\n",
    "balance = 2  # regularization -- balances denoising vs preserving bandwidth\n",
    "# set up an impulse PSF to operate Wiener as denoiser only, no deconvolution\n",
    "psf = np.zeros([filt_len,filt_len])\n",
    "psf[filt_len//2, filt_len//2] = 1.\n",
    "\n",
    "x = next(iter(dataloader_val))[0] # get a batch of images\n",
    "forwardModel = ForwardModel(0.2)\n",
    "y = forwardModel(x)\n",
    "\n",
    "# denoise with neural network\n",
    "with torch.no_grad():\n",
    "    xhat = net(y.to(device))\n",
    "\n",
    "# denoise with Wiener filter\n",
    "xhat_wiener = restoration.wiener(y[0].cpu().squeeze().numpy(),psf,balance)\n",
    "\n",
    "# calculate MAE, MSE, Pearson, PSNR, SSIM metrics\n",
    "x_np = x[0].cpu().numpy().squeeze()\n",
    "xhat_np = xhat[0].cpu().numpy().squeeze()\n",
    "\n",
    "MSE_neural = MSE(x_np,xhat_np)\n",
    "MAE_neural = MAE(x_np,xhat_np)\n",
    "pearson_neural = Pearson(x_np,xhat_np)\n",
    "SSIM_neural = SSIM(x_np,xhat_np)\n",
    "PSNR_neural = PSNR(x_np,xhat_np)\n",
    "\n",
    "MSE_wiener = MSE(x_np, xhat_wiener)\n",
    "MAE_wiener = MAE(x_np, xhat_wiener)\n",
    "pearson_wiener = Pearson(x_np, xhat_wiener)\n",
    "SSIM_wiener = SSIM(x_np, xhat_wiener)\n",
    "PSNR_wiener = PSNR(x_np, xhat_wiener)\n",
    "\n",
    "print('method \\t MSE \\t MAE \\tPearson\\t SSIM \\t PSNR (dB)')\n",
    "print(f'Neural \\t {MSE_neural:0.3f} \\t {MAE_neural:0.3f} \\t {pearson_neural:0.3f} \\t {SSIM_neural:0.3f} \\t {PSNR_neural:0.3f}')\n",
    "print(f'Wiener \\t {MSE_wiener:0.3f} \\t {MAE_wiener:0.3f} \\t {pearson_wiener:0.3f} \\t {SSIM_wiener:0.3f} \\t {PSNR_wiener:0.3f}')\n",
    "\n",
    "# plot results\n",
    "plt.subplot(221)\n",
    "plt.imshow(x[0].cpu().squeeze())\n",
    "plt.axis('off')\n",
    "plt.title('clean')\n",
    "plt.subplot(222)\n",
    "plt.imshow(y[0].cpu().squeeze())\n",
    "plt.axis('off')\n",
    "plt.title('noisy')\n",
    "plt.subplot(223)\n",
    "plt.imshow(xhat_wiener)\n",
    "plt.axis('off')\n",
    "plt.title('wiener')\n",
    "plt.subplot(224)\n",
    "plt.imshow(xhat[0].cpu().squeeze())\n",
    "plt.axis('off')\n",
    "plt.title('neural net')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5207ae84-616f-4676-9627-b7b4e1e61fc5",
   "metadata": {},
   "source": [
    "## Uncertainty quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a5959e-f0cf-41b5-9998-d5ad1d609f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN neural network with uncertainty\n",
    "class CNN_uncert( nn.Module ):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1,32,kernel_size=3,padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32,32,kernel_size=3,padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32,32,kernel_size=3,padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32,32,kernel_size=3,padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32,2,kernel_size=3,padding=1) )\n",
    "\n",
    "        # initialize\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='leaky_relu')\n",
    "                nn.init.constant_(m.bias,0)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        xhat = out[:,0]\n",
    "        variance = F.softplus(out[:,1])\n",
    "        return xhat, variance\n",
    "    \n",
    "net = CNN_uncert()\n",
    "net = net.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d13242-f5f9-4f90-98b7-682ab10531e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple u-net with batchnorm and variance estimate\n",
    "class UNet_uncert( nn.Module ):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.downconv1 = nn.Sequential(nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 3, padding=1),nn.BatchNorm2d(32))\n",
    "        self.downconv2 = nn.Sequential(nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, padding=1),nn.BatchNorm2d(32))\n",
    "        self.downconv3 = nn.Sequential(nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, padding=1),nn.BatchNorm2d(32))\n",
    "        self.downconv4 = nn.Sequential(nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, padding=1),nn.BatchNorm2d(32))\n",
    "\n",
    "        self.upconv1 = nn.Sequential(\n",
    "                    nn.ConvTranspose2d(in_channels = 32, out_channels = 32, kernel_size = 3, padding=1, stride=2, output_padding=1),\n",
    "                    nn.BatchNorm2d(32) )\n",
    "        self.upconv2 = nn.Sequential(\n",
    "                    nn.ConvTranspose2d(in_channels = 64, out_channels = 32, kernel_size = 3, padding=1, stride=2, output_padding=1),\n",
    "                    nn.BatchNorm2d(32) )\n",
    "        self.upconv3 = nn.Sequential(\n",
    "                        nn.ConvTranspose2d(in_channels = 64, out_channels = 32, kernel_size = 3, padding=1, stride=2, output_padding=1),\n",
    "                        nn.BatchNorm2d(32) )\n",
    "        self.upconv4 = nn.Sequential(\n",
    "                        nn.ConvTranspose2d(in_channels = 64, out_channels = 32, kernel_size = 3, padding=1, stride=2, output_padding=1),\n",
    "                        nn.BatchNorm2d(32) )\n",
    "\n",
    "        self.outconv_a = nn.Conv2d(in_channels = 64, out_channels = 1, kernel_size = 3, padding=1)\n",
    "        self.outconv_b = nn.Conv2d(in_channels = 64, out_channels = 1, kernel_size = 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        featdown1 = F.leaky_relu(self.downconv1(x))\n",
    "        featdown2 = F.leaky_relu(self.downconv2(F.max_pool2d(featdown1,2)))\n",
    "        featdown3 = F.leaky_relu(self.downconv3(F.max_pool2d(featdown2,2)))\n",
    "        featdown4 = F.leaky_relu(self.downconv4(F.max_pool2d(featdown3,2)))\n",
    "\n",
    "        featup1 = F.leaky_relu(self.upconv1(F.max_pool2d(featdown4,2)))\n",
    "        skip1 = torch.cat((featdown4, featup1),1)\n",
    "        featup2 = F.leaky_relu(self.upconv2(skip1))\n",
    "        skip2 = torch.cat((featdown3, featup2),1)\n",
    "        featup3 = F.leaky_relu(self.upconv3(skip2))\n",
    "        skip3 = torch.cat((featdown2, featup3),1)\n",
    "        featup4 = F.leaky_relu(self.upconv4(skip3))\n",
    "        skip4 = torch.cat((featdown1, featup4),1)\n",
    "        xhat = self.outconv_a(skip4)\n",
    "        var = F.softplus(self.outconv_b(skip4))\n",
    "\n",
    "        return xhat, var\n",
    "        \n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.)\n",
    "\n",
    "net = UNet_uncert().to(device)\n",
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407e48df-5f27-4642-a352-5b964346a66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test run a noisy image through the network\n",
    "x = next(iter(dataloader_train))[0] # get a batch of images\n",
    "forwardModel = ForwardModel(0.2)\n",
    "y = forwardModel(x)\n",
    "with torch.no_grad():\n",
    "    xhat, var = net(y.to(device)) # run through network\n",
    "\n",
    "plt.subplot(141)\n",
    "plt.imshow(x[0].squeeze())\n",
    "plt.title('original')\n",
    "plt.subplot(142)\n",
    "plt.imshow(y[0].squeeze())\n",
    "plt.title('noisy')\n",
    "plt.subplot(143)\n",
    "plt.imshow(xhat[0].cpu().squeeze())\n",
    "plt.title('network output')\n",
    "plt.subplot(144)\n",
    "plt.imshow(var[0].cpu().squeeze())\n",
    "plt.title('uncertainty')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a40bf9b-6c6e-4994-a770-c20529648fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heteroscedastic loss function\n",
    "class HeteroscedasticLoss( nn.Module ):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.eps=1e-5\n",
    "\n",
    "    def forward(self, mean, var, target):\n",
    "        var += self.eps\n",
    "        loss = 0.5*torch.mean( torch.abs(mean - target)/var + torch.log(var))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8f2504-47c7-49e9-b2c5-56aff6edf961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supervised training\n",
    "forwardModel = ForwardModel(0.2)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_fn_het = HeteroscedasticLoss()\n",
    "loss_fn_L2 = nn.L1Loss()\n",
    "het_loss_warmup_iters = 4000\n",
    "\n",
    "loss_train_vec = []\n",
    "\n",
    "# training loop\n",
    "n_epochs = 100\n",
    "i_iter = 0\n",
    "for epoch in range(1,n_epochs):\n",
    "    for data, label in dataloader_train:\n",
    "    \n",
    "        x = data.to(device)\n",
    "        y = forwardModel(x)\n",
    "        xhat,var = net(y)\n",
    "\n",
    "        # compare recovered image with measurement\n",
    "        loss_het = loss_fn_het(xhat,var,x)\n",
    "        loss_L2 = loss_fn_L2(xhat,x)\n",
    "        alpha=0\n",
    "        alpha = i_iter / het_loss_warmup_iters\n",
    "        if alpha > 1:\n",
    "            alpha = 1\n",
    "        \n",
    "        loss = alpha * loss_het + (1-alpha)*loss_L2\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(net.parameters(),0.01)\n",
    "        optimizer.step()\n",
    "    \n",
    "        loss_train_vec += [loss.item()]\n",
    "        i_iter += 1\n",
    "    \n",
    "        # plot training/validation loss curves\n",
    "        if (i_iter+1) % 10 == 0:\n",
    "            clear_output(wait=True)\n",
    "            plt.figure(figsize=[8,8])\n",
    "            plt.subplot(241)\n",
    "            plt.imshow(x[0].cpu().squeeze())\n",
    "            plt.subplot(242)\n",
    "            plt.imshow(y[0].cpu().squeeze())\n",
    "            plt.subplot(243)\n",
    "            plt.imshow(xhat[0].detach().cpu().squeeze())\n",
    "            plt.subplot(244)\n",
    "            plt.imshow(var[0].detach().cpu().squeeze(),vmin=0,vmax=1)\n",
    "            plt.subplot(212)\n",
    "            plt.plot(loss_train_vec)\n",
    "            plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572601b9-8ee2-428b-bbe8-f21c9335c1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if uncertainty increases with noise level\n",
    "# render a panel of images\n",
    "noiseLevels = [0.1, 0.2, 0.4, 0.6, 0.8]\n",
    "plt.figure(figsize=[4,7])\n",
    "for ind in range(5):\n",
    "    x = next(iter(dataloader_val))[0] # get a batch of images\n",
    "    forwardModel = ForwardModel(noiseLevels[ind])\n",
    "    y = forwardModel(x)\n",
    "    with torch.no_grad():\n",
    "        xhat,var = net(y.to(device))\n",
    "        \n",
    "    plt.subplot(5,4,ind*4+1)\n",
    "    plt.imshow(x[ind].squeeze())\n",
    "    plt.axis('off')\n",
    "    if ind==0:\n",
    "        plt.title('original')\n",
    "    plt.subplot(5,4,ind*4+2)\n",
    "    plt.imshow(y[ind].squeeze())\n",
    "    plt.axis('off')\n",
    "    if ind==0:\n",
    "        plt.title('noisy')\n",
    "    plt.subplot(5,4,ind*4+3)\n",
    "    plt.imshow(xhat[ind].cpu().squeeze())\n",
    "    plt.axis('off')\n",
    "    if ind==0:\n",
    "        plt.title('denoised')\n",
    "    plt.subplot(5,4,ind*4+4)\n",
    "    plt.imshow(var[ind].cpu().squeeze(),vmin=0,vmax=2)\n",
    "    plt.axis('off')\n",
    "    if ind==0:\n",
    "        plt.title('variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caa18e2-1580-4206-b220-7cd6a213ebf6",
   "metadata": {},
   "source": [
    "## Unsupervised Learning (noise2void)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7086bf-8e01-4f7a-a6bc-d8f9c0a22096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsupervised training (noise2void)\n",
    "#optimizer = torch.optim.SGD(net.parameters(),lr=0.1,momentum=0.9)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "loss_train_vec = []\n",
    "\n",
    "# training loop\n",
    "n_epochs = 100000\n",
    "for epoch in range(1,n_epochs):\n",
    "    ind = randint(0,len(dataset_train)-1)\n",
    "    x = dataset_train[ind][0].to(device)\n",
    "\n",
    "    \n",
    "    y = forwardModel(x)\n",
    "    \n",
    "    # noise2void masking: select 10% of pixels for noise2void loss\n",
    "    mask = torch.rand_like(y) > 0.5\n",
    "    y_masked = y.clone() # use clone operation; otherwise, modifications to y_masked will overwrite data in y\n",
    "    y_masked[mask] = 0.\n",
    "    \n",
    "    xhat = net(y_masked.unsqueeze(0))\n",
    "\n",
    "    # compare recovered image with measurement\n",
    "    #loss = loss_fn(xhat.flatten()[mask.flatten()],y.flatten()[mask.flatten()])\n",
    "    loss = loss_fn(xhat*mask,y*mask)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_train_vec += [loss.item()]\n",
    "    \n",
    "    # plot training/validation loss curves\n",
    "    if (epoch+1) % 200 == 0:\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=[8,8])\n",
    "        plt.subplot(241)\n",
    "        plt.imshow(x.cpu().squeeze())\n",
    "        plt.subplot(242)\n",
    "        plt.imshow(y_masked.cpu().squeeze())\n",
    "        plt.subplot(243)\n",
    "        plt.imshow(xhat.detach().cpu().squeeze())\n",
    "        plt.subplot(244)\n",
    "        plt.imshow(mask.cpu().squeeze())\n",
    "        plt.subplot(212)\n",
    "        plt.plot(loss_train_vec)\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce86619",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = randint(0,(len(y)-1))\n",
    "plt.subplot(131)\n",
    "plt.imshow(x_val[ind].cpu().squeeze())\n",
    "plt.title('blurry')\n",
    "plt.subplot(132)\n",
    "plt.imshow(yhat[ind].cpu().squeeze())\n",
    "plt.title('estimate')\n",
    "plt.subplot(133)\n",
    "plt.imshow(y[ind].cpu().squeeze())\n",
    "plt.title('ground truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f152e658-36d1-4994-8ddb-58c796ae89de",
   "metadata": {},
   "source": [
    "## Try a different dataset\n",
    "[Link to PolyU Real World Noisy Images Dataset](https://github.com/csjunxu/PolyU-Real-World-Noisy-Images-Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3527030e-ade4-4113-b8ef-fd6b7d707f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "url = \"https://github.com/csjunxu/PolyU-Real-World-Noisy-Images-Dataset/archive/refs/heads/master.zip\"\n",
    "\n",
    "file_path=\"polyu.zip\"\n",
    "if not os.path.exists(file_path):\n",
    "    urllib.request.urlretrieve(url,file_path)\n",
    "\n",
    "extract_to = \"polyu\"\n",
    "if not os.path.exists(extract_to):\n",
    "    os.makedirs(extract_to)\n",
    "\n",
    "    with zipfile.ZipFile(file_path,'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7adeec1-9d36-496a-a6b4-8ec882c2e649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a data class\n",
    "# data files are of the form prefix_mean.jpg, prefix_real.jpg\n",
    "# where the _real.jpg is the noisy image and _mean.jpg is the clean reference\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_path = \"polyu/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages\"\n",
    "\n",
    "class PolyUDataset( Dataset ):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # get a listing of all the files\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        listing = sorted(os.listdir(data_path))\n",
    "        self.files_noisy = [f for f in listing if f.endswith(\"_real.JPG\")]\n",
    "        self.files_clean = [f for f in listing if f.endswith(\"_mean.JPG\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files_noisy)\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        filename_x = self.files_noisy[ind]\n",
    "        x = Image.open(os.path.join(self.data_path,filename_x))\n",
    "\n",
    "        filename_y = self.files_clean[ind]\n",
    "        y = Image.open(os.path.join(self.data_path,filename_y))\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "            y = self.transform(y)\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataset_polyu = PolyUDataset(data_path, transform)\n",
    "\n",
    "n = len(dataset_polyu)\n",
    "print(f\"PolyU dataset loaded with n={n} images\")\n",
    "\n",
    "batch_size=16\n",
    "dataloader_polyu = DataLoader(dataset_polyu, batch_size, shuffle=True)\n",
    "\n",
    "data = next(iter(dataloader_polyu))\n",
    "x = data[0]\n",
    "y = data[1]\n",
    "\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.subplot(121)\n",
    "plt.imshow(x[0].cpu().squeeze().permute(1,2,0))\n",
    "plt.title('noisy')\n",
    "plt.subplot(122)\n",
    "plt.imshow(y[0].cpu().squeeze().permute(1,2,0))\n",
    "plt.title('clean')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
