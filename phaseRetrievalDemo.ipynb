{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a781f6-db9f-4373-848d-afba87c85931",
   "metadata": {},
   "source": [
    "# Phase Retrieval demo.\n",
    "\n",
    "SPIE Short course on Machine Learning for Image Restoration.  \n",
    "Author: Jesse Wilson (jesse.wilson@colostate.edu).\n",
    "\n",
    "Walk through training and evaluation of a convolutional network for phase retrieval from coherent diffractive imaging. This code is provided for educational purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119a8d68-7c96-4af1-bf62-3e260a8713bb",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a07d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from torchvision.transforms.functional import gaussian_blur\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from torch.fft import fft2, fftshift, ifft2, ifftshift\n",
    "\n",
    "# get available GPU \n",
    "# supports NVIDIA (CUDA), Intel (XPU), and Apple (MPS)\n",
    "# (CAUTION: AI-generated code -- NOT validated on all systems!)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif hasattr(torch,\"xpu\") and torch.xpu_is_available():\n",
    "    device = torch.device(\"xpu:0\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Selected device: {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be014ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a dataset\n",
    "\n",
    "batch_size=64\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Pad(18)])\n",
    "dataset_train = datasets.MNIST(root='data',train=True,download=True,transform=transform)\n",
    "dataset_val = datasets.MNIST(root='data',train=False,download=True, transform = transform)\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train,batch_size=64)\n",
    "\n",
    "\n",
    "print(dataset_train[0][0].shape)\n",
    "plt.imshow(dataset_train[0][0].squeeze())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b66aec7-2fb1-4a11-b919-1d429826f7dd",
   "metadata": {},
   "source": [
    "# Set up the forward model\n",
    "Functions and class definition to simulate passing a coherent beam through a circular aperture, then a phase object, then propagating to the Fraunhofer plane, and recording an amplitude-only diffraction pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d822f9-185f-4382-9460-97bc624ece1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAperture( radius ):\n",
    "    x=np.linspace(1,64,64)-32\n",
    "    y=np.linspace(1,64,64)-32\n",
    "    X,Y = np.meshgrid(x,y)\n",
    "    \n",
    "    aperture = X**2+Y**2 <= radius**2\n",
    "    return aperture\n",
    "\n",
    "def createPhaseObject( img, aperture ):\n",
    "    # set up a complex object in which the image is encoded with phase\n",
    "    # and is surrounded by an opaque aperture\n",
    "    nbatch = img.shape[0]\n",
    "    obj = torch.exp( 3.14j/18.*img )\n",
    "    obj = obj * aperture\n",
    "\n",
    "    return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fe4799-7ebc-4a51-b112-281da9431ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardModel( obj ):\n",
    "    # calculate a diffraction patern of a complex object using the Fourier transform\n",
    "    diffr_complex = fftshift(fft2(ifftshift(obj)))\n",
    "    \n",
    "    diffr_phase = diffr_complex.angle() # phase of diffraction patern (not measurable)\n",
    "    diffr_abs = diffr_complex.abs()   # amplitude of diffraction pattern (seen by imaging sensor)\n",
    "\n",
    "    return( diffr_abs, diffr_phase )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3b0aad-80d0-476b-8a12-1aeb090ae094",
   "metadata": {},
   "source": [
    "# Neural network definition and quick passthrough test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5542dd1-bc8f-4092-b313-b0e8d461061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 64x64 encoder decoder network with fully connected bottleneck\n",
    "# uses strided convs instead of conv->maxpool\n",
    "# uses strided transposed convs instead of upsampling -> conv\n",
    "# note: started modifying my previous code for encoder-decoder, got stuck on \n",
    "#       exact syntax and details for flattening/unflattening, so the initial\n",
    "#       draft of the bottleneck was generated by prompting MS Copilot:\n",
    "#       \n",
    "#       Simple pytorch code for encoder-decoder with a fully-connected bottleneck. \n",
    "#       Should take 64x64 images, use strided convolutions for downsampling \n",
    "#       and transposed convs for upsampling, and have a 7x7 bottleneck.\n",
    "#\n",
    "class EncDec( nn.Module ):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.nFilt = 32\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv2d(1,self.nFilt,kernel_size=3,stride=2), # 64x64 -> 31x31\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(self.nFilt,self.nFilt,kernel_size=3,stride=2), # 31x31 -> 15x15\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(self.nFilt,self.nFilt,kernel_size=3,stride=2), # 15x15 -> 7x7\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(self.nFilt,self.nFilt,kernel_size=3,stride=2), # 7x7 -> 3x3\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.nFilt,self.nFilt,kernel_size=3,stride=2), # 3x3 -> 7x7\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(self.nFilt,self.nFilt,kernel_size=3,stride=2), # 7x7 -> 15x15\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(self.nFilt,self.nFilt,kernel_size=3,stride=2), # 15x15 -> 31x31\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(self.nFilt,1,kernel_size=3,stride=2,output_padding=1), # 15x15 -> 64x64\n",
    "        )\n",
    "\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(3*3*self.nFilt, 3*3*self.nFilt),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(3*3*self.nFilt, 3*3*self.nFilt),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Unflatten(1,(self.nFilt,3,3))\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='leaky_relu')\n",
    "                nn.init.constant_(m.bias,0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.enc(x)\n",
    "        x = self.bottleneck(x)\n",
    "        x = self.dec(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "net = EncDec().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7db8ec-9a8c-4f46-a287-0bc729b223a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick test: generate a random object, diffraction pattern, and pass through network\n",
    "ind = randint(0,len(dataset_val)-1)\n",
    "img = dataset_val[ind][0].to(device).unsqueeze(0)\n",
    "\n",
    "aperture = createAperture(16)\n",
    "aperture = torch.Tensor(aperture).to(device)\n",
    "\n",
    "obj_complex = createPhaseObject(img, aperture)\n",
    "diffr_abs, diffr_phase  = forwardModel(obj_complex)\n",
    "\n",
    "obj_phase_est = net(diffr_abs)\n",
    "\n",
    "# plot the results\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.subplot(231)\n",
    "plt.imshow(obj_complex.abs().cpu().squeeze())\n",
    "plt.axis('off')\n",
    "plt.title('abs(object)')\n",
    "\n",
    "plt.subplot(232)\n",
    "plt.imshow(obj_complex.angle().cpu().squeeze())\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.title('phase(object)')\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.imshow(obj_phase_est.detach().cpu().squeeze())\n",
    "plt.axis('off')\n",
    "plt.title('network output')\n",
    "\n",
    "plt.subplot(234)\n",
    "plt.imshow((diffr_abs.cpu().squeeze()))\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.title('(abs(diffraction pattern))')\n",
    "\n",
    "plt.subplot(235)\n",
    "plt.imshow(diffr_phase.cpu().squeeze())\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.title('phase(diffraction pattern)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d7830f-7b98-4b35-9d98-6d61d32e5ad0",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53de62e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.003)\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "# load a limited set of the training data onto the GPU\n",
    "nTrain = 5000\n",
    "\n",
    "# calculate complex objects\n",
    "aperture = createAperture(16)\n",
    "aperture = torch.Tensor(aperture).to(device)\n",
    "\n",
    "\n",
    "loss_train_vec = []\n",
    "\n",
    "# training loop\n",
    "n_epochs = 1500\n",
    "for epoch in range(1,n_epochs):\n",
    "    for img, label in dataloader_train:\n",
    "        img = img.to(device)\n",
    "        obj = createPhaseObject( img, aperture )\n",
    "        diffr_abs, diffr_phase = forwardModel(obj)\n",
    "\n",
    "        obj_phase_est = net(diffr_abs)\n",
    "    \n",
    "        loss = loss_fn(obj_phase_est,obj.angle())\n",
    "        optimizer.zero_grad()\n",
    "     \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        loss_train_vec += [loss.item()]\n",
    "    \n",
    "    # plot training/validation loss curves\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=[8,8])\n",
    "    plt.subplot(221)\n",
    "    plt.imshow(obj[0].angle().cpu().squeeze())\n",
    "    plt.subplot(222)\n",
    "    plt.imshow(obj_phase_est[0].detach().cpu().squeeze())\n",
    "    plt.subplot(212)\n",
    "    plt.plot(loss_train_vec)\n",
    "    #plt.plot(loss_val_vec)\n",
    "    plt.legend(['train','val'])\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c23de6-32e5-4255-aeb0-6f43cf842e27",
   "metadata": {},
   "source": [
    "# Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0c67ab-be2a-461f-b5d9-73fb6690497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your turn: change one thing above and run it again. A few ideas\n",
    "# - change pupil diameter\n",
    "# - change learning rate\n",
    "# - change neural network architecture\n",
    "# - add validation tracking to the training loop\n",
    "# - test on out-of-distribution images\n",
    "# - add noise to simulated image\n",
    "# - compare with Feinup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68f2654-21e8-45af-a62a-380c8180fa08",
   "metadata": {},
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac8c9ef-a6e1-413e-bcbd-59196be79e13",
   "metadata": {},
   "source": [
    "## Physics-informed (unsupervised) training\n",
    "Note: this proof of concept code _barely_ works, and will need some fine tuning and experimentation to be robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7d06f0-c58a-4c43-b6c5-63eb8367c125",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientMagnitude(nn.Module):\n",
    "    def __init__(self,dev):\n",
    "        super().__init__()\n",
    "        self.sobel_x = torch.tensor([[1.,0.,-1.],[2.,0.,-2.],[1.,0.,1.]]).unsqueeze(0).unsqueeze(0).to(dev)\n",
    "        self.sobel_y = torch.tensor([[1.,2.,1.],[0.,0.,0.],[-1.,-2.,-1.]]).unsqueeze(0).unsqueeze(0).to(dev)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # calculate summed magnitude of the gradients of an image\n",
    "        grad_x = F.conv2d(x,self.sobel_x)\n",
    "        grad_y = F.conv2d(y,self.sobel_y)\n",
    "    \n",
    "        absgrad = -torch.abs(grad_x)**2 + torch.abs(grad_y)**2\n",
    "        return torch.mean(absgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8887b7d-5e93-439b-901f-980612878ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsupervised training\n",
    "# given measured diffraction amplitude pattern, find object phase such that estimated diffraction pattern matches measured\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "#loss_fn_gradmag = GradientMagnitude(device)\n",
    "\n",
    "\n",
    "# calculate complex objects\n",
    "aperture = createAperture(16)\n",
    "aperture = torch.Tensor(aperture).to(device)\n",
    "\n",
    "\n",
    "loss_train_vec = []\n",
    "#loss_val_vec = []\n",
    "\n",
    "# training loop\n",
    "n_epochs = 1500\n",
    "for epoch in range(1,n_epochs):\n",
    "    for img, label in dataloader_train:\n",
    "        img = img.to(device)\n",
    "        obj = createPhaseObject( img, aperture )\n",
    "        diffr_abs, diffr_phase = forwardModel(obj)\n",
    "\n",
    "        obj_phase_est = net(diffr_abs)\n",
    "        \n",
    "        obj_est = createPhaseObject(obj_phase_est, aperture)\n",
    "        diffr_abs_est, diffr_phase_est = forwardModel(obj_est)\n",
    "\n",
    "        # unsupervised loss (does NOT make use of object phase)\n",
    "        #loss = loss_fn(diffr_abs_est,diffr_abs) + 0.1*torch.mean(torch.abs(obj_phase_est*(1-aperture))) + 0.1*torch.mean(torch.relu(-1*obj_phase_est))+0.001*loss_fn_gradmag(obj_phase_est)\n",
    "        #loss = loss_fn(diffr_abs_est,diffr_abs) + 0.00001*loss_fn_gradmag(obj_phase_est)\n",
    "        loss  = loss_fn(diffr_abs_est,diffr_abs) + 0.1*torch.mean(obj_phase_est**2) + torch.mean(torch.relu(-1*obj_phase_est))\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step(loss)\n",
    "    \n",
    "        loss_train_vec += [loss.item()]\n",
    "    \n",
    " \n",
    "    # plot training/validation loss curves\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=[8,8])\n",
    "    plt.subplot(221)\n",
    "    plt.imshow(obj[0].angle().cpu().squeeze(),vmin=0,vmax=0.2)\n",
    "    plt.subplot(222)\n",
    "    plt.imshow(obj_phase_est[0].detach().cpu().squeeze())\n",
    "    plt.colorbar()\n",
    "    plt.subplot(212)\n",
    "    plt.plot(loss_train_vec)\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb58fbd-0af8-424c-9a80-2bd42260eaf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
